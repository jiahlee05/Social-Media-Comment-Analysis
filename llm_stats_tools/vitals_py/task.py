"""
Vitals for Python: LLM Evaluation Framework
"""

from typing import Optional, Callable, Dict, List, Any, Literal
from dataclasses import dataclass, field
import pandas as pd
import numpy as np
from anthropic import Anthropic
import json
import os
from datetime import datetime
from pathlib import Path
import uuid


@dataclass
class Sample:
    """í‰ê°€ ìƒ˜í”Œ"""
    id: str
    input: str
    target: str
    epoch: int = 1
    result: Optional[str] = None
    score: Optional[str] = None
    solver_metadata: Optional[Dict] = None
    scorer_metadata: Optional[Dict] = None


@dataclass
class TaskMetrics:
    """í‰ê°€ ë©”íŠ¸ë¦­"""
    accuracy: float = 0.0
    partial_credit_rate: float = 0.0
    total_samples: int = 0
    correct: int = 0
    partial: int = 0
    incorrect: int = 0


class Task:
    """
    LLM í‰ê°€ Task

    R vitals íŒ¨í‚¤ì§€ì˜ Python êµ¬í˜„
    """

    def __init__(
        self,
        dataset: pd.DataFrame,
        solver: Callable,
        scorer: Callable,
        name: Optional[str] = None,
        epochs: int = 1,
        log_dir: Optional[str] = None
    ):
        """
        Args:
            dataset: 'input'ê³¼ 'target' ì»¬ëŸ¼ì„ ê°€ì§„ DataFrame
            solver: ì…ë ¥ì„ ë°›ì•„ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
            scorer: ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
            name: Task ì´ë¦„
            epochs: ê° ìƒ˜í”Œì„ ë°˜ë³µí•  íšŸìˆ˜
            log_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.dataset = dataset
        self.solver_func = solver
        self.scorer_func = scorer
        self.name = name or f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.epochs = epochs
        self.log_dir = log_dir or os.getenv("VITALS_LOG_DIR", "./vitals_logs")

        self.samples: List[Sample] = []
        self.metrics: Optional[TaskMetrics] = None
        self._solver_executed = False
        self._scorer_executed = False

        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.log_dir).mkdir(parents=True, exist_ok=True)

    def solve(self, **kwargs) -> 'Task':
        """Solver ì‹¤í–‰"""
        print(f"ğŸ”§ Solving task: {self.name}")
        print(f"   Samples: {len(self.dataset)} Ã— {self.epochs} epochs")

        # epochë³„ë¡œ ìƒ˜í”Œ í™•ì¥
        expanded_samples = []
        for epoch in range(1, self.epochs + 1):
            for idx, row in self.dataset.iterrows():
                sample = Sample(
                    id=f"{idx}_{epoch}",
                    input=row['input'],
                    target=row['target'],
                    epoch=epoch
                )
                expanded_samples.append(sample)

        # Solver ì‹¤í–‰
        inputs = [s.input for s in expanded_samples]
        solver_results = self.solver_func(inputs, **kwargs)

        # ê²°ê³¼ ì €ì¥
        for i, sample in enumerate(expanded_samples):
            sample.result = solver_results['result'][i]
            if 'solver_metadata' in solver_results:
                sample.solver_metadata = solver_results['solver_metadata'][i]

        self.samples = expanded_samples
        self._solver_executed = True

        print(f"   âœ“ Solved {len(self.samples)} samples")
        return self

    def score(self, **kwargs) -> 'Task':
        """Scorer ì‹¤í–‰"""
        if not self._solver_executed:
            raise ValueError("solve()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”")

        print(f"ğŸ“Š Scoring task: {self.name}")

        # Scorer ì‹¤í–‰
        scorer_results = self.scorer_func(self.samples, **kwargs)

        # ê²°ê³¼ ì €ì¥
        for i, sample in enumerate(self.samples):
            sample.score = scorer_results['score'][i]
            if 'scorer_metadata' in scorer_results:
                sample.scorer_metadata = scorer_results['scorer_metadata'][i]

        self._scorer_executed = True
        print(f"   âœ“ Scored {len(self.samples)} samples")
        return self

    def measure(self) -> 'Task':
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not self._scorer_executed:
            raise ValueError("score()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”")

        scores = [s.score for s in self.samples]

        correct = sum(1 for s in scores if s == 'C')
        partial = sum(1 for s in scores if s == 'P')
        incorrect = sum(1 for s in scores if s == 'I')
        total = len(scores)

        self.metrics = TaskMetrics(
            accuracy=correct / total if total > 0 else 0.0,
            partial_credit_rate=partial / total if total > 0 else 0.0,
            total_samples=total,
            correct=correct,
            partial=partial,
            incorrect=incorrect
        )

        print(f"\nğŸ“ˆ Metrics:")
        print(f"   Accuracy: {self.metrics.accuracy:.2%}")
        print(f"   Correct: {correct}/{total}")
        print(f"   Partial: {partial}/{total}")
        print(f"   Incorrect: {incorrect}/{total}")

        return self

    def log(self) -> str:
        """ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë¡œê¹…"""
        log_file = Path(self.log_dir) / f"{self.name}_{uuid.uuid4().hex[:8]}.json"

        log_data = {
            "name": self.name,
            "created_at": datetime.now().isoformat(),
            "epochs": self.epochs,
            "metrics": self.metrics.__dict__ if self.metrics else None,
            "samples": [
                {
                    "id": s.id,
                    "input": s.input,
                    "target": s.target,
                    "result": s.result,
                    "score": s.score,
                    "epoch": s.epoch,
                    "solver_metadata": s.solver_metadata,
                    "scorer_metadata": s.scorer_metadata
                }
                for s in self.samples
            ]
        }

        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ Logged to: {log_file}")
        return str(log_file)

    def get_samples(self) -> pd.DataFrame:
        """ìƒ˜í”Œì„ DataFrameìœ¼ë¡œ ë°˜í™˜"""
        return pd.DataFrame([
            {
                'id': s.id,
                'epoch': s.epoch,
                'input': s.input,
                'target': s.target,
                'result': s.result,
                'score': s.score
            }
            for s in self.samples
        ])

    def eval(self, view: bool = False, **kwargs) -> 'Task':
        """ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.solve(**kwargs)
        self.score(**kwargs)
        self.measure()
        self.log()

        if view:
            self.view()

        return self

    def view(self):
        """ê²°ê³¼ ì‹œê°í™” (ê°„ë‹¨í•œ ì¶œë ¥)"""
        print("\n" + "="*70)
        print(f"Task: {self.name}")
        print("="*70)

        df = self.get_samples()
        print(df.to_string())

        if self.metrics:
            print(f"\n{self.metrics}")


# Solver ìƒì„± í•¨ìˆ˜ë“¤
def generate(client: Anthropic, model: str = "claude-sonnet-4-5-20250929"):
    """
    ê¸°ë³¸ generate solver

    Args:
        client: Anthropic client
        model: ì‚¬ìš©í•  Claude ëª¨ë¸

    Returns:
        Solver í•¨ìˆ˜
    """
    def solver(inputs: List[str], **kwargs) -> Dict[str, List]:
        results = []
        metadata = []

        for inp in inputs:
            response = client.messages.create(
                model=model,
                max_tokens=2000,
                messages=[{"role": "user", "content": inp}]
            )

            results.append(response.content[0].text)
            metadata.append({
                'model': model,
                'usage': response.usage.model_dump()
            })

        return {
            'result': results,
            'solver_metadata': metadata
        }

    return solver
