# Session 4: í…ìŠ¤íŠ¸ ë¶„ì„ & ì›Œë“œí´ë¼ìš°ë“œ

**ìˆ˜ì—… ì‹œê°„:** 2ì‹œê°„
**ëª©í‘œ:** ì†Œì…œ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ ë„ì¶œí•˜ê¸°

---

## ðŸ“‹ ìˆ˜ì—… ëª©ì°¨

1. **í…ìŠ¤íŠ¸ ë¶„ì„ ìž…ë¬¸** (15ë¶„)
2. **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬** (30ë¶„)
3. **ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±** (30ë¶„)
4. **ê°ì„± ë¶„ì„ (Sentiment Analysis)** (30ë¶„)
5. **ì‹¤ì „: ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„° ë¶„ì„** (15ë¶„)

---

## 1. í…ìŠ¤íŠ¸ ë¶„ì„ ìž…ë¬¸ (15ë¶„)

### ì™œ í…ìŠ¤íŠ¸ ë¶„ì„ì¸ê°€?

**ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì—°êµ¬ì˜ í•µì‹¬ ë°ì´í„°:**
- ì†Œì…œ ë¯¸ë””ì–´ ëŒ“ê¸€/í¬ìŠ¤íŠ¸
- ë‰´ìŠ¤ ê¸°ì‚¬
- ê³ ê° ë¦¬ë·°
- ì„¤ë¬¸ ì‘ë‹µ
- ì¸í„°ë·° ë‚´ìš©

### í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ í•  ìˆ˜ ìžˆëŠ” ê²ƒ

1. **ë¹ˆë„ ë¶„ì„**: ì–´ë–¤ ë‹¨ì–´ê°€ ìžì£¼ ë‚˜ì˜¤ëŠ”ê°€?
2. **ê°ì„± ë¶„ì„**: ê¸ì •ì ? ë¶€ì •ì ? ì¤‘ë¦½ì ?
3. **í† í”½ ëª¨ë¸ë§**: ì£¼ìš” ì£¼ì œëŠ”?
4. **íŠ¸ë Œë“œ ë¶„ì„**: ì‹œê°„ì— ë”°ë¥¸ ë³€í™”
5. **ë¹„êµ ë¶„ì„**: ê·¸ë£¹ ê°„ ì°¨ì´

---

## 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (30ë¶„)

### ì „ì²˜ë¦¬ê°€ í•„ìš”í•œ ì´ìœ 

ì›ì‹œ í…ìŠ¤íŠ¸ â†’ ë¶„ì„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜

**ì˜ˆì‹œ:**
```
ì›ë³¸: "I LOVE this product!!! ðŸ˜ðŸ˜ðŸ˜ #BestEver"
ì „ì²˜ë¦¬ í›„: ["love", "product", "best"]
```

### ì£¼ìš” ì „ì²˜ë¦¬ ë‹¨ê³„

#### 1. ì†Œë¬¸ìž ë³€í™˜
```python
text = "Hello World!"
text_lower = text.lower()  # "hello world!"
```

#### 2. í† í°í™” (Tokenization)
```python
from nltk.tokenize import word_tokenize

text = "This is a sentence."
tokens = word_tokenize(text)
# ['This', 'is', 'a', 'sentence', '.']
```

#### 3. ë¶ˆìš©ì–´ ì œê±° (Stopwords Removal)
```python
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))
filtered = [word for word in tokens if word.lower() not in stop_words]
# ['sentence', '.']
```

**í•œê¸€ ë¶ˆìš©ì–´:**
```python
korean_stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼']
```

#### 4. íŠ¹ìˆ˜ë¬¸ìž ì œê±°
```python
import re

text = "Hello, World! @#$%"
cleaned = re.sub(r'[^a-zA-Z0-9\s]', '', text)
# "Hello World"
```

#### 5. ì–´ê°„ ì¶”ì¶œ (Stemming) / í‘œì œì–´ ì¶”ì¶œ (Lemmatization)
```python
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Stemming
stemmer.stem("running")  # "run"
stemmer.stem("ran")      # "ran"

# Lemmatization (ë” ì •í™•)
lemmatizer.lemmatize("running", pos='v')  # "run"
lemmatizer.lemmatize("ran", pos='v')      # "run"
```

### ì¢…í•© ì „ì²˜ë¦¬ í•¨ìˆ˜

```python
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

def preprocess_text(text):
    # ì†Œë¬¸ìž ë³€í™˜
    text = text.lower()

    # íŠ¹ìˆ˜ë¬¸ìž ì œê±°
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    # í† í°í™”
    tokens = word_tokenize(text)

    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]

    # í‘œì œì–´ ì¶”ì¶œ
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]

    return tokens
```

---

## 3. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± (30ë¶„)

### ì›Œë“œí´ë¼ìš°ë“œëž€?

í…ìŠ¤íŠ¸ì—ì„œ ìžì£¼ ë“±ìž¥í•˜ëŠ” ë‹¨ì–´ë¥¼ í¬ê¸°ë¡œ ì‹œê°í™”

### ê¸°ë³¸ ì›Œë“œí´ë¼ìš°ë“œ

```python
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# í…ìŠ¤íŠ¸ ë°ì´í„°
text = "data analysis python python data visualization statistics"

# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
wordcloud = WordCloud(width=800, height=400,
                      background_color='white').generate(text)

# ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()
```

### ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
wordcloud = WordCloud(
    width=1200,
    height=600,
    background_color='white',
    colormap='viridis',      # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
    max_words=100,            # ìµœëŒ€ ë‹¨ì–´ ìˆ˜
    relative_scaling=0.5,     # ë‹¨ì–´ í¬ê¸° ìƒëŒ€ì  ì¡°ì •
    min_font_size=10,         # ìµœì†Œ ê¸€ìž í¬ê¸°
    stopwords=stop_words,     # ë¶ˆìš©ì–´
    contour_width=2,          # ìœ¤ê³½ì„ 
    contour_color='steelblue'
).generate(text)
```

### ë¹ˆë„ìˆ˜ ê¸°ë°˜ ì›Œë“œí´ë¼ìš°ë“œ

```python
from collections import Counter

# ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
words = text.split()
word_freq = Counter(words)

# ë¹ˆë„ìˆ˜ë¡œ ì›Œë“œí´ë¼ìš°ë“œ
wordcloud = WordCloud(width=800, height=400,
                      background_color='white').generate_from_frequencies(word_freq)
```

### ëª¨ì–‘ ë§ˆìŠ¤í¬ ì ìš©

```python
from PIL import Image
import numpy as np

# ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ (ì˜ˆ: í•˜íŠ¸ ëª¨ì–‘)
mask = np.array(Image.open('heart_mask.png'))

wordcloud = WordCloud(mask=mask, background_color='white',
                      contour_width=1, contour_color='red').generate(text)
```

---

## 4. ê°ì„± ë¶„ì„ (30ë¶„)

### ê°ì„± ë¶„ì„ì´ëž€?

í…ìŠ¤íŠ¸ì˜ ê°ì •/ì˜ê²¬ íŒë‹¨: ê¸ì •, ë¶€ì •, ì¤‘ë¦½

**í™œìš© ì˜ˆ:**
- ì œí’ˆ ë¦¬ë·° ë¶„ì„
- ë¸Œëžœë“œ í‰íŒ ëª¨ë‹ˆí„°ë§
- ì†Œì…œ ë¯¸ë””ì–´ ë°˜ì‘ ì¶”ì 
- ì—¬ë¡  ì¡°ì‚¬

### VADER (Valence Aware Dictionary and sEntiment Reasoner)

ì†Œì…œ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— ìµœì í™”ëœ ê°ì„± ë¶„ì„ ë„êµ¬

```python
from nltk.sentiment import SentimentIntensityAnalyzer

sia = SentimentIntensityAnalyzer()

# ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ì„
text = "I absolutely love this product! It's amazing!"
scores = sia.polarity_scores(text)
print(scores)
# {'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.875}
```

**ì ìˆ˜ í•´ì„:**
- `compound`: -1 (ë§¤ìš° ë¶€ì •) ~ +1 (ë§¤ìš° ê¸ì •)
  - â‰¥ 0.05: ê¸ì •
  - â‰¤ -0.05: ë¶€ì •
  - ê·¸ ì™¸: ì¤‘ë¦½

### ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ë¶„ì„

```python
import pandas as pd

# ë°ì´í„°í”„ë ˆìž„ì— ê°ì„± ì ìˆ˜ ì¶”ê°€
df['sentiment'] = df['comment'].apply(lambda x: sia.polarity_scores(x)['compound'])

# ë²”ì£¼í™”
def categorize_sentiment(score):
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)

# ë¶„í¬ í™•ì¸
print(df['sentiment_category'].value_counts())
```

### ì‹œê°í™”

```python
import seaborn as sns

# ê°ì„± ë¶„í¬
plt.figure(figsize=(10, 6))
sns.countplot(x='sentiment_category', data=df, palette='Set2')
plt.title('Sentiment Distribution')
plt.show()

# ê°ì„± ì ìˆ˜ ížˆìŠ¤í† ê·¸ëž¨
plt.figure(figsize=(10, 6))
plt.hist(df['sentiment'], bins=50, color='skyblue', edgecolor='black')
plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Neutral')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.title('Distribution of Sentiment Scores')
plt.legend()
plt.show()
```

---

## 5. ì‹¤ì „: ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„° ë¶„ì„ (15ë¶„)

### ì¢…í•© ë¶„ì„ ì˜ˆì œ

```python
import pandas as pd
from wordcloud import WordCloud
from nltk.sentiment import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv('../datasets/social_media_comments.csv')

# 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
df['cleaned_text'] = df['comment'].apply(preprocess_text)

# 2. ì›Œë“œí´ë¼ìš°ë“œ
all_words = ' '.join([' '.join(words) for words in df['cleaned_text']])
wordcloud = WordCloud(width=1200, height=600,
                      background_color='white').generate(all_words)

plt.figure(figsize=(15, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Most Common Words in Comments', fontsize=18, fontweight='bold')
plt.savefig('wordcloud.png', dpi=300, bbox_inches='tight')
plt.show()

# 3. ê°ì„± ë¶„ì„
sia = SentimentIntensityAnalyzer()
df['sentiment_score'] = df['comment'].apply(lambda x: sia.polarity_scores(x)['compound'])
df['sentiment'] = df['sentiment_score'].apply(categorize_sentiment)

# 4. í”Œëž«í¼ë³„ ê°ì„± ë¹„êµ
sentiment_by_platform = pd.crosstab(df['platform'], df['sentiment'], normalize='index') * 100

plt.figure(figsize=(12, 6))
sentiment_by_platform.plot(kind='bar', stacked=True, colormap='RdYlGn')
plt.title('Sentiment Distribution by Platform')
plt.ylabel('Percentage')
plt.legend(title='Sentiment')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 5. ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ë¶„ì„
positive = df[df['sentiment'] == 'Positive']
negative = df[df['sentiment'] == 'Negative']

# ê¸ì • ì›Œë“œí´ë¼ìš°ë“œ
pos_words = ' '.join([' '.join(words) for words in positive['cleaned_text']])
wc_positive = WordCloud(background_color='white', colormap='Greens').generate(pos_words)

# ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ
neg_words = ' '.join([' '.join(words) for words in negative['cleaned_text']])
wc_negative = WordCloud(background_color='white', colormap='Reds').generate(neg_words)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))
axes[0].imshow(wc_positive)
axes[0].set_title('Positive Comments', fontsize=14, fontweight='bold')
axes[0].axis('off')

axes[1].imshow(wc_negative)
axes[1].set_title('Negative Comments', fontsize=14, fontweight='bold')
axes[1].axis('off')

plt.tight_layout()
plt.savefig('sentiment_wordclouds.png', dpi=300, bbox_inches='tight')
plt.show()
```

---

## ðŸ’¡ í•µì‹¬ ìš”ì•½

### í…ìŠ¤íŠ¸ ë¶„ì„ ì›Œí¬í”Œë¡œìš°

1. **ë°ì´í„° ìˆ˜ì§‘** â†’ ì†Œì…œ ë¯¸ë””ì–´, ë¦¬ë·°, ì„¤ë¬¸ ë“±
2. **ì „ì²˜ë¦¬** â†’ ì†Œë¬¸ìž, í† í°í™”, ë¶ˆìš©ì–´ ì œê±°
3. **ë¶„ì„** â†’ ë¹ˆë„, ì›Œë“œí´ë¼ìš°ë“œ, ê°ì„±
4. **ì‹œê°í™”** â†’ ì›Œë“œí´ë¼ìš°ë“œ, ì°¨íŠ¸
5. **í•´ì„** â†’ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

### NLTK ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)

```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')
```

### í•œê¸€ í…ìŠ¤íŠ¸ ë¶„ì„

```python
# KoNLPy ì‚¬ìš© (í•œê¸€ í˜•íƒœì†Œ ë¶„ì„)
from konlpy.tag import Okt

okt = Okt()
text = "ë°ì´í„° ë¶„ì„ì€ ì •ë§ ìž¬ë¯¸ìžˆìŠµë‹ˆë‹¤"
nouns = okt.nouns(text)  # ëª…ì‚¬ ì¶”ì¶œ
# ['ë°ì´í„°', 'ë¶„ì„', 'ìž¬ë¯¸']
```

---

## ðŸ¤– AI í˜‘ì—… ê°€ì´ë“œ

```
1. "ì´ ëŒ“ê¸€ ë°ì´í„°ì—ì„œ ê°€ìž¥ ìžì£¼ ì–¸ê¸‰ë˜ëŠ” ë¶ˆë§Œì‚¬í•­ì„ ì°¾ì•„ì¤˜"

2. "ì›Œë“œí´ë¼ìš°ë“œë¥¼ íŠ¹ì • ëª¨ì–‘(ì˜ˆ: ë¡œê³ )ìœ¼ë¡œ ë§Œë“¤ê³  ì‹¶ì–´. ì–´ë–»ê²Œ í•´?"

3. "VADERê°€ ì´ ë¬¸ìž¥ì„ ìž˜ëª» ë¶„ë¥˜í•œ ê²ƒ ê°™ì•„. ë‹¤ë¥¸ ë°©ë²•ì´ ìžˆì„ê¹Œ?"

4. "ê¸ì • ëŒ“ê¸€ê³¼ ë¶€ì • ëŒ“ê¸€ì˜ í‚¤ì›Œë“œ ì°¨ì´ë¥¼ ë¹„êµí•˜ëŠ” ì½”ë“œ ì§œì¤˜"

5. "ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°„ëŒ€ë³„ë¡œ ì¶”ì í•˜ëŠ” íŠ¸ë Œë“œ ì°¨íŠ¸ ë§Œë“¤ê¸°"
```

---

## ðŸ“ ë‹¤ìŒ ìˆ˜ì—… ì˜ˆê³ 

**Session 5: ì‹¤í—˜ ë°ì´í„° ì²˜ë¦¬ & ë³´ê³ ì„œ ìž‘ì„±**
- ì‹¤í—˜ ë°ì´í„° í´ë¦¬ë‹
- ì¢…í•© ë¶„ì„ íŒŒì´í”„ë¼ì¸
- ìžë™í™”ëœ ë³´ê³ ì„œ ìƒì„±
- ìµœì¢… í”„ë¡œì íŠ¸

---

**í…ìŠ¤íŠ¸ì— ìˆ¨ê²¨ì§„ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°œê²¬í•˜ì„¸ìš”! ðŸ’¬**
